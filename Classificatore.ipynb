{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****CON LOOP GIGANTE****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datas from: /Users/Riccardo/Desktop/Progetto Scalogrammi/Scalogrammi\n",
      "\n",
      "Loading complete!\n",
      "Datasets dimension:\n",
      "X shape: 618\n",
      "y shape: 618\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 1: - Random Seed: 2627\n",
      "100%|██████████| 220/220 [05:59<00:00,  1.63s/trial, best loss: -0.7903225806451613]\n",
      "{'C': 5.689703078164446, 'gamma': 0.37841046370029335, 'kernel': 1}\n",
      "{'C': 5.689703078164446, 'gamma': 0.37841046370029335, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7778\n",
      "\u001b[1m\u001b[32mThe best accuracy on the validation set is \u001b[0m\u001b[1m\u001b[31m0.7778 \u001b[0m\u001b[1m\u001b[32mwith hyperparameters: \u001b[0m\u001b[1m\u001b[31m{'C': 5.689703078164446, 'gamma': 0.37841046370029335, 'kernel': 'poly'}\u001b[0m\u001b[1m\u001b[32mwith random seed: \u001b[0m\u001b[1m\u001b[32m2627\u001b[0m\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8710\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;208mTHAT'S A NEW BEST!!\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 2: - Random Seed: 6273\n",
      "100%|██████████| 220/220 [05:45<00:00,  1.57s/trial, best loss: -0.7903225806451613]\n",
      "{'C': 2.7785511593726913, 'gamma': 0.057359719589462406, 'kernel': 1}\n",
      "{'C': 2.7785511593726913, 'gamma': 0.057359719589462406, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8758\n",
      "\u001b[1m\u001b[32mThe best accuracy on the validation set is \u001b[0m\u001b[1m\u001b[31m0.8758 \u001b[0m\u001b[1m\u001b[32mwith hyperparameters: \u001b[0m\u001b[1m\u001b[31m{'C': 2.7785511593726913, 'gamma': 0.057359719589462406, 'kernel': 'poly'}\u001b[0m\u001b[1m\u001b[32mwith random seed: \u001b[0m\u001b[1m\u001b[32m6273\u001b[0m\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8000\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 3: - Random Seed: 3568\n",
      "100%|██████████| 220/220 [05:41<00:00,  1.55s/trial, best loss: -0.832258064516129]\n",
      "{'C': 0.5574302372728505, 'gamma': 0.3286904387810772, 'kernel': 1}\n",
      "{'C': 0.5574302372728505, 'gamma': 0.3286904387810772, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8431\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8194\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 4: - Random Seed: 6068\n",
      "100%|██████████| 220/220 [05:48<00:00,  1.59s/trial, best loss: -0.7903225806451613]\n",
      "{'C': 0.17151257988807314, 'gamma': 7.691581790771325, 'kernel': 1}\n",
      "{'C': 0.17151257988807314, 'gamma': 7.691581790771325, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7843\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8581\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 5: - Random Seed: 3134\n",
      "100%|██████████| 220/220 [05:46<00:00,  1.57s/trial, best loss: -0.8129032258064516]\n",
      "{'C': 1.0297634831055507, 'gamma': 14.288303397423231, 'kernel': 1}\n",
      "{'C': 1.0297634831055507, 'gamma': 14.288303397423231, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8301\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7548\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 6: - Random Seed: 4163\n",
      "100%|██████████| 220/220 [05:39<00:00,  1.54s/trial, best loss: -0.8129032258064516]\n",
      "{'C': 3.006836229685678, 'gamma': 0.11727115428642007, 'kernel': 1}\n",
      "{'C': 3.006836229685678, 'gamma': 0.11727115428642007, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8039\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8129\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 7: - Random Seed: 2781\n",
      "100%|██████████| 220/220 [05:41<00:00,  1.55s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 0.3427081576590431, 'gamma': 0.2171730260696475, 'kernel': 1}\n",
      "{'C': 0.3427081576590431, 'gamma': 0.2171730260696475, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8366\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8581\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 8: - Random Seed: 7930\n",
      "100%|██████████| 220/220 [05:44<00:00,  1.57s/trial, best loss: -0.8193548387096774]\n",
      "{'C': 3.764180544055172, 'gamma': 0.09993190961266894, 'kernel': 1}\n",
      "{'C': 3.764180544055172, 'gamma': 0.09993190961266894, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7843\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.9097\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;5;208mTHAT'S A NEW BEST!!\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 9: - Random Seed: 6402\n",
      "100%|██████████| 220/220 [05:42<00:00,  1.56s/trial, best loss: -0.8032258064516129]\n",
      "{'C': 2.3762047893127503, 'gamma': 14.878315238711128, 'kernel': 1}\n",
      "{'C': 2.3762047893127503, 'gamma': 14.878315238711128, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8301\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8065\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 10: - Random Seed: 181\n",
      "100%|██████████| 220/220 [05:47<00:00,  1.58s/trial, best loss: -0.8096774193548387]\n",
      "{'C': 2.6659702464061645, 'gamma': 0.18611926670467951, 'kernel': 1}\n",
      "{'C': 2.6659702464061645, 'gamma': 0.18611926670467951, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8431\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8000\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 11: - Random Seed: 493\n",
      "100%|██████████| 220/220 [05:46<00:00,  1.57s/trial, best loss: -0.8]            \n",
      "{'C': 0.06872999068436624, 'gamma': 1.108557434331177, 'kernel': 1}\n",
      "{'C': 0.06872999068436624, 'gamma': 1.108557434331177, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8562\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8645\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 12: - Random Seed: 8456\n",
      "100%|██████████| 220/220 [05:45<00:00,  1.57s/trial, best loss: -0.8096774193548387]\n",
      "{'C': 2.7011484953613087, 'gamma': 6.565155806240011, 'kernel': 1}\n",
      "{'C': 2.7011484953613087, 'gamma': 6.565155806240011, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.6928\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8387\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 13: - Random Seed: 1123\n",
      "100%|██████████| 220/220 [05:49<00:00,  1.59s/trial, best loss: -0.7967741935483871]\n",
      "{'C': 0.34298854403237794, 'gamma': 1.4181601252823737, 'kernel': 1}\n",
      "{'C': 0.34298854403237794, 'gamma': 1.4181601252823737, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8431\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8710\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 14: - Random Seed: 1607\n",
      "100%|██████████| 220/220 [05:43<00:00,  1.56s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 0.1741366966821974, 'gamma': 0.09470097606596227, 'kernel': 1}\n",
      "{'C': 0.1741366966821974, 'gamma': 0.09470097606596227, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8301\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8839\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 15: - Random Seed: 6462\n",
      "100%|██████████| 220/220 [05:46<00:00,  1.57s/trial, best loss: -0.7967741935483871]\n",
      "{'C': 1.2084218358326382, 'gamma': 1.8126236062794057, 'kernel': 1}\n",
      "{'C': 1.2084218358326382, 'gamma': 1.8126236062794057, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7908\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.9032\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 16: - Random Seed: 8201\n",
      "100%|██████████| 220/220 [05:45<00:00,  1.57s/trial, best loss: -0.8]             \n",
      "{'C': 0.7425810280031421, 'gamma': 2.0690747029754846, 'kernel': 1}\n",
      "{'C': 0.7425810280031421, 'gamma': 2.0690747029754846, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8627\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8968\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 17: - Random Seed: 7928\n",
      "100%|██████████| 220/220 [05:43<00:00,  1.56s/trial, best loss: -0.7903225806451613]\n",
      "{'C': 0.08450222071613273, 'gamma': 0.12833396800536367, 'kernel': 1}\n",
      "{'C': 0.08450222071613273, 'gamma': 0.12833396800536367, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7778\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8387\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 18: - Random Seed: 5697\n",
      "100%|██████████| 220/220 [05:42<00:00,  1.56s/trial, best loss: -0.8193548387096774]\n",
      "{'C': 0.10995794722983206, 'gamma': 1.1296468946437697, 'kernel': 1}\n",
      "{'C': 0.10995794722983206, 'gamma': 1.1296468946437697, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7778\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7677\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 19: - Random Seed: 8569\n",
      "100%|██████████| 220/220 [05:44<00:00,  1.57s/trial, best loss: -0.8290322580645161]\n",
      "{'C': 0.06716176062269415, 'gamma': 6.308860379773139, 'kernel': 1}\n",
      "{'C': 0.06716176062269415, 'gamma': 6.308860379773139, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7647\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7871\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 20: - Random Seed: 7441\n",
      "100%|██████████| 220/220 [05:39<00:00,  1.55s/trial, best loss: -0.835483870967742]\n",
      "{'C': 0.07744746219089588, 'gamma': 13.310212252926231, 'kernel': 1}\n",
      "{'C': 0.07744746219089588, 'gamma': 13.310212252926231, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7320\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8323\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 21: - Random Seed: 7079\n",
      "100%|██████████| 220/220 [05:45<00:00,  1.57s/trial, best loss: -0.8193548387096774]\n",
      "{'C': 0.6004887169310835, 'gamma': 0.12929779347115655, 'kernel': 1}\n",
      "{'C': 0.6004887169310835, 'gamma': 0.12929779347115655, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8301\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8258\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 22: - Random Seed: 6532\n",
      "100%|██████████| 220/220 [05:46<00:00,  1.57s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 2.5968593513613225, 'gamma': 0.6950811920535765, 'kernel': 1}\n",
      "{'C': 2.5968593513613225, 'gamma': 0.6950811920535765, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7843\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8323\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 23: - Random Seed: 6776\n",
      "100%|██████████| 220/220 [05:49<00:00,  1.59s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 0.14657821990946057, 'gamma': 0.14576367816873723, 'kernel': 1}\n",
      "{'C': 0.14657821990946057, 'gamma': 0.14576367816873723, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7647\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8581\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 24: - Random Seed: 1699\n",
      "100%|██████████| 220/220 [05:43<00:00,  1.56s/trial, best loss: -0.8064516129032259]\n",
      "{'C': 0.22940488713078908, 'gamma': 0.5766548962834452, 'kernel': 1}\n",
      "{'C': 0.22940488713078908, 'gamma': 0.5766548962834452, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7451\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8452\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 25: - Random Seed: 7664\n",
      "100%|██████████| 220/220 [05:41<00:00,  1.55s/trial, best loss: -0.8225806451612904]\n",
      "{'C': 0.18584192718729942, 'gamma': 0.8678829147328987, 'kernel': 1}\n",
      "{'C': 0.18584192718729942, 'gamma': 0.8678829147328987, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7843\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8516\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 26: - Random Seed: 4219\n",
      "100%|██████████| 220/220 [05:44<00:00,  1.56s/trial, best loss: -0.8064516129032258]\n",
      "{'C': 0.784681511290541, 'gamma': 0.08846358477341924, 'kernel': 1}\n",
      "{'C': 0.784681511290541, 'gamma': 0.08846358477341924, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7320\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.9032\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 27: - Random Seed: 4665\n",
      "100%|██████████| 220/220 [05:46<00:00,  1.57s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 4.41646735170234, 'gamma': 1.210338251421956, 'kernel': 1}\n",
      "{'C': 4.41646735170234, 'gamma': 1.210338251421956, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8562\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.9097\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 28: - Random Seed: 4685\n",
      "100%|██████████| 220/220 [05:49<00:00,  1.59s/trial, best loss: -0.8032258064516129]\n",
      "{'C': 4.673724710116524, 'gamma': 0.17187614410166507, 'kernel': 1}\n",
      "{'C': 4.673724710116524, 'gamma': 0.17187614410166507, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7255\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8968\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 29: - Random Seed: 6532\n",
      "100%|██████████| 220/220 [05:48<00:00,  1.58s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 0.06126268075623215, 'gamma': 6.159549393701278, 'kernel': 1}\n",
      "{'C': 0.06126268075623215, 'gamma': 6.159549393701278, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7843\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8323\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 30: - Random Seed: 3973\n",
      "100%|██████████| 220/220 [05:45<00:00,  1.57s/trial, best loss: -0.7967741935483871]\n",
      "{'C': 0.4487164600185218, 'gamma': 10.573554743534043, 'kernel': 1}\n",
      "{'C': 0.4487164600185218, 'gamma': 10.573554743534043, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.6863\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8452\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 31: - Random Seed: 3769\n",
      "100%|██████████| 220/220 [05:38<00:00,  1.54s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 0.05992842988750252, 'gamma': 0.199278413060225, 'kernel': 1}\n",
      "{'C': 0.05992842988750252, 'gamma': 0.199278413060225, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7386\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8968\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 32: - Random Seed: 2207\n",
      "100%|██████████| 220/220 [05:43<00:00,  1.56s/trial, best loss: -0.8225806451612904]\n",
      "{'C': 0.07321599592769426, 'gamma': 6.93842227375582, 'kernel': 1}\n",
      "{'C': 0.07321599592769426, 'gamma': 6.93842227375582, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7582\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8516\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 33: - Random Seed: 7420\n",
      "100%|██████████| 220/220 [05:42<00:00,  1.56s/trial, best loss: -0.7806451612903226]\n",
      "{'C': 4.4374773635259706, 'gamma': 0.5308828888220679, 'kernel': 1}\n",
      "{'C': 4.4374773635259706, 'gamma': 0.5308828888220679, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8562\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7871\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 34: - Random Seed: 4710\n",
      "100%|██████████| 220/220 [05:40<00:00,  1.55s/trial, best loss: -0.864516129032258]\n",
      "{'C': 9.616164263446676, 'gamma': 0.1942703394965593, 'kernel': 1}\n",
      "{'C': 9.616164263446676, 'gamma': 0.1942703394965593, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7843\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.9032\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 35: - Random Seed: 9094\n",
      "100%|██████████| 220/220 [05:42<00:00,  1.56s/trial, best loss: -0.8064516129032259]\n",
      "{'C': 0.1966974342539941, 'gamma': 0.39858909165410533, 'kernel': 1}\n",
      "{'C': 0.1966974342539941, 'gamma': 0.39858909165410533, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7647\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7484\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 36: - Random Seed: 5898\n",
      "100%|██████████| 220/220 [05:46<00:00,  1.57s/trial, best loss: -0.832258064516129]\n",
      "{'C': 1.4343055431134306, 'gamma': 10.804173056210225, 'kernel': 1}\n",
      "{'C': 1.4343055431134306, 'gamma': 10.804173056210225, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8758\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8387\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 37: - Random Seed: 9291\n",
      "100%|██████████| 220/220 [05:45<00:00,  1.57s/trial, best loss: -0.7967741935483871]\n",
      "{'C': 4.26595500883335, 'gamma': 4.901941502000605, 'kernel': 1}\n",
      "{'C': 4.26595500883335, 'gamma': 4.901941502000605, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7778\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8452\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 38: - Random Seed: 3898\n",
      "100%|██████████| 220/220 [05:48<00:00,  1.58s/trial, best loss: -0.8064516129032259]\n",
      "{'C': 0.0607221419151775, 'gamma': 0.1775103645147279, 'kernel': 1}\n",
      "{'C': 0.0607221419151775, 'gamma': 0.1775103645147279, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8497\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8323\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 39: - Random Seed: 9503\n",
      "100%|██████████| 220/220 [05:44<00:00,  1.57s/trial, best loss: -0.7709677419354839]\n",
      "{'C': 0.1931937719978655, 'gamma': 9.622618729134622, 'kernel': 1}\n",
      "{'C': 0.1931937719978655, 'gamma': 9.622618729134622, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8366\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8323\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 40: - Random Seed: 7819\n",
      "100%|██████████| 220/220 [05:46<00:00,  1.58s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 0.052665446312333025, 'gamma': 0.058512947768679005, 'kernel': 1}\n",
      "{'C': 0.052665446312333025, 'gamma': 0.058512947768679005, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7778\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8065\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 41: - Random Seed: 4452\n",
      "100%|██████████| 220/220 [05:47<00:00,  1.58s/trial, best loss: -0.8129032258064516]\n",
      "{'C': 1.1299117789991697, 'gamma': 3.941337376699959, 'kernel': 1}\n",
      "{'C': 1.1299117789991697, 'gamma': 3.941337376699959, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8431\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8387\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 42: - Random Seed: 5722\n",
      "100%|██████████| 220/220 [05:45<00:00,  1.57s/trial, best loss: -0.8032258064516129]\n",
      "{'C': 0.0931722353715311, 'gamma': 0.14110796558588026, 'kernel': 1}\n",
      "{'C': 0.0931722353715311, 'gamma': 0.14110796558588026, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8235\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8258\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 43: - Random Seed: 9341\n",
      "100%|██████████| 220/220 [05:46<00:00,  1.57s/trial, best loss: -0.8225806451612904]\n",
      "{'C': 0.27692153586860235, 'gamma': 4.424884487168173, 'kernel': 1}\n",
      "{'C': 0.27692153586860235, 'gamma': 4.424884487168173, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8431\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8710\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 44: - Random Seed: 7664\n",
      "100%|██████████| 220/220 [05:41<00:00,  1.55s/trial, best loss: -0.8225806451612904]\n",
      "{'C': 3.130542375306231, 'gamma': 4.005227877281258, 'kernel': 1}\n",
      "{'C': 3.130542375306231, 'gamma': 4.005227877281258, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7843\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8516\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 45: - Random Seed: 7572\n",
      "100%|██████████| 220/220 [05:56<00:00,  1.62s/trial, best loss: -0.7935483870967742]\n",
      "{'C': 0.06348999576543649, 'gamma': 4.358214656292856, 'kernel': 1}\n",
      "{'C': 0.06348999576543649, 'gamma': 4.358214656292856, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8497\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8323\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 46: - Random Seed: 7180\n",
      "100%|██████████| 220/220 [05:47<00:00,  1.58s/trial, best loss: -0.8419354838709677]\n",
      "{'C': 0.0830957275884607, 'gamma': 8.015398104190695, 'kernel': 1}\n",
      "{'C': 0.0830957275884607, 'gamma': 8.015398104190695, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.9346\n",
      "\u001b[1m\u001b[32mThe best accuracy on the validation set is \u001b[0m\u001b[1m\u001b[31m0.9346 \u001b[0m\u001b[1m\u001b[32mwith hyperparameters: \u001b[0m\u001b[1m\u001b[31m{'C': 0.0830957275884607, 'gamma': 8.015398104190695, 'kernel': 'poly'}\u001b[0m\u001b[1m\u001b[32mwith random seed: \u001b[0m\u001b[1m\u001b[32m7180\u001b[0m\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8710\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 47: - Random Seed: 8256\n",
      "100%|██████████| 220/220 [07:42<00:00,  2.10s/trial, best loss: -0.7806451612903226]\n",
      "{'C': 6.958726156179626, 'gamma': 5.365490737252184, 'kernel': 1}\n",
      "{'C': 6.958726156179626, 'gamma': 5.365490737252184, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8693\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8516\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 48: - Random Seed: 2501\n",
      "100%|██████████| 220/220 [07:19<00:00,  2.00s/trial, best loss: -0.7548387096774194]\n",
      "{'C': 0.055484168007255655, 'gamma': 0.19964909188779995, 'kernel': 1}\n",
      "{'C': 0.055484168007255655, 'gamma': 0.19964909188779995, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7908\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8387\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 49: - Random Seed: 8576\n",
      "100%|██████████| 220/220 [07:20<00:00,  2.00s/trial, best loss: -0.7677419354838709]\n",
      "{'C': 0.09174324508673862, 'gamma': 1.6133683246340023, 'kernel': 1}\n",
      "{'C': 0.09174324508673862, 'gamma': 1.6133683246340023, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7908\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7484\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 50: - Random Seed: 4796\n",
      "100%|██████████| 220/220 [07:25<00:00,  2.03s/trial, best loss: -0.8161290322580645]\n",
      "{'C': 18.910671885677196, 'gamma': 12.853788584086125, 'kernel': 1}\n",
      "{'C': 18.910671885677196, 'gamma': 12.853788584086125, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7516\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8516\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 51: - Random Seed: 2518\n",
      "100%|██████████| 220/220 [07:17<00:00,  1.99s/trial, best loss: -0.7677419354838709]\n",
      "{'C': 0.5141525160181503, 'gamma': 6.999163291277063, 'kernel': 1}\n",
      "{'C': 0.5141525160181503, 'gamma': 6.999163291277063, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7974\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8774\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 52: - Random Seed: 287\n",
      "100%|██████████| 220/220 [07:18<00:00,  1.99s/trial, best loss: -0.7741935483870968]\n",
      "{'C': 0.5477677580429308, 'gamma': 1.1289773277556199, 'kernel': 1}\n",
      "{'C': 0.5477677580429308, 'gamma': 1.1289773277556199, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.6993\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8516\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 53: - Random Seed: 7515\n",
      "100%|██████████| 220/220 [07:08<00:00,  1.95s/trial, best loss: -0.7903225806451613]\n",
      "{'C': 3.969090993356956, 'gamma': 0.14189769505246874, 'kernel': 1}\n",
      "{'C': 3.969090993356956, 'gamma': 0.14189769505246874, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8562\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8129\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 54: - Random Seed: 5684\n",
      "100%|██████████| 220/220 [07:14<00:00,  1.98s/trial, best loss: -0.8129032258064516]\n",
      "{'C': 0.23491266785678994, 'gamma': 0.8674532923674632, 'kernel': 1}\n",
      "{'C': 0.23491266785678994, 'gamma': 0.8674532923674632, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7908\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7355\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 55: - Random Seed: 3069\n",
      "100%|██████████| 220/220 [07:12<00:00,  1.97s/trial, best loss: -0.7870967741935484]\n",
      "{'C': 0.6925277427140524, 'gamma': 0.05730205907999752, 'kernel': 1}\n",
      "{'C': 0.6925277427140524, 'gamma': 0.05730205907999752, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8562\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8903\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 56: - Random Seed: 4525\n",
      "100%|██████████| 220/220 [07:13<00:00,  1.97s/trial, best loss: -0.8290322580645161]\n",
      "{'C': 5.326752684501796, 'gamma': 0.31097104341879345, 'kernel': 1}\n",
      "{'C': 5.326752684501796, 'gamma': 0.31097104341879345, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8562\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7871\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 57: - Random Seed: 9522\n",
      "100%|██████████| 220/220 [07:20<00:00,  2.00s/trial, best loss: -0.7935483870967742]\n",
      "{'C': 1.310208503064873, 'gamma': 0.09385448540120045, 'kernel': 1}\n",
      "{'C': 1.310208503064873, 'gamma': 0.09385448540120045, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8497\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8258\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 58: - Random Seed: 3593\n",
      "100%|██████████| 220/220 [07:02<00:00,  1.92s/trial, best loss: -0.7935483870967742]\n",
      "{'C': 1.0244321134090153, 'gamma': 0.05611038451043702, 'kernel': 1}\n",
      "{'C': 1.0244321134090153, 'gamma': 0.05611038451043702, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7647\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7871\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 59: - Random Seed: 8649\n",
      "100%|██████████| 220/220 [06:54<00:00,  1.88s/trial, best loss: -0.8612903225806452]\n",
      "{'C': 1.0483426731772862, 'gamma': 18.33091191075512, 'kernel': 1}\n",
      "{'C': 1.0483426731772862, 'gamma': 18.33091191075512, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8627\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7871\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 60: - Random Seed: 647\n",
      "100%|██████████| 220/220 [07:01<00:00,  1.92s/trial, best loss: -0.8290322580645162]\n",
      "{'C': 0.34540985069096974, 'gamma': 0.4732037253080697, 'kernel': 1}\n",
      "{'C': 0.34540985069096974, 'gamma': 0.4732037253080697, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7516\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8645\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 61: - Random Seed: 1672\n",
      "100%|██████████| 220/220 [06:54<00:00,  1.88s/trial, best loss: -0.7935483870967742]\n",
      "{'C': 1.5750106908737402, 'gamma': 5.031462818909004, 'kernel': 1}\n",
      "{'C': 1.5750106908737402, 'gamma': 5.031462818909004, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.8170\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8387\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 62: - Random Seed: 8370\n",
      "100%|██████████| 220/220 [06:59<00:00,  1.91s/trial, best loss: -0.8129032258064516]\n",
      "{'C': 0.7099876378553504, 'gamma': 11.717011435439659, 'kernel': 1}\n",
      "{'C': 0.7099876378553504, 'gamma': 11.717011435439659, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7516\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8516\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 63: - Random Seed: 3049\n",
      "100%|██████████| 220/220 [07:05<00:00,  1.93s/trial, best loss: -0.7967741935483871]\n",
      "{'C': 0.11286502392625294, 'gamma': 7.4757582968441225, 'kernel': 1}\n",
      "{'C': 0.11286502392625294, 'gamma': 7.4757582968441225, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7059\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.8452\u001b[0m\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "\n",
      "Iteration 64: - Random Seed: 4348\n",
      " 32%|███▏      | 71/220 [02:22<04:58,  2.01s/trial, best loss: -0.8129032258064516]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Riccardo\\Desktop\\Progetto Scalogrammi\\Classificatore.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: - Random Seed: \u001b[39m\u001b[39m{\u001b[39;00mrandom_seed\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39m# Optimization\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m best \u001b[39m=\u001b[39m fmin(fn \u001b[39m=\u001b[39;49m \u001b[39mlambda\u001b[39;49;00m param: objective(param, X_train_temp, y_train_temp), \u001b[39m#Pass X_train, y_train as parameters to objective\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m                 space \u001b[39m=\u001b[39;49m param_grid_bs, algo \u001b[39m=\u001b[39;49m tpe\u001b[39m.\u001b[39;49msuggest, max_evals \u001b[39m=\u001b[39;49m \u001b[39m220\u001b[39;49m, trials \u001b[39m=\u001b[39;49m bayes_trials)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m \u001b[39m# index best parameters\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39mprint\u001b[39m(best)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[0;32m    541\u001b[0m         fn,\n\u001b[0;32m    542\u001b[0m         space,\n\u001b[0;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[0;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[0;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[0;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[0;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[0;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[0;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[0;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[0;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[0;32m    556\u001b[0m     )\n\u001b[0;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[0;32m    672\u001b[0m     fn,\n\u001b[0;32m    673\u001b[0m     space,\n\u001b[0;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[0;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[0;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[0;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[0;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[0;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[0;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[0;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[0;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[0;32m    689\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[0;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[0;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[0;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[0;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[1;32mc:\\Users\\Riccardo\\Desktop\\Progetto Scalogrammi\\Classificatore.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: - Random Seed: \u001b[39m\u001b[39m{\u001b[39;00mrandom_seed\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39m# Optimization\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m best \u001b[39m=\u001b[39m fmin(fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m param: objective(param, X_train_temp, y_train_temp), \u001b[39m#Pass X_train, y_train as parameters to objective\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m                 space \u001b[39m=\u001b[39m param_grid_bs, algo \u001b[39m=\u001b[39m tpe\u001b[39m.\u001b[39msuggest, max_evals \u001b[39m=\u001b[39m \u001b[39m220\u001b[39m, trials \u001b[39m=\u001b[39m bayes_trials)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m \u001b[39m# index best parameters\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39mprint\u001b[39m(best)\n",
      "\u001b[1;32mc:\\Users\\Riccardo\\Desktop\\Progetto Scalogrammi\\Classificatore.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(param, X, y):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     svc \u001b[39m=\u001b[39m SVC(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparam)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     scores \u001b[39m=\u001b[39m cross_val_score(svc, X, y, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, error_score\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mraise\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# Cross-validation = 5, n_jobs = -1 means use all cores avaiables\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     \u001b[39m# best\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Riccardo/Desktop/Progetto%20Scalogrammi/Classificatore.ipynb#W1sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     best_score \u001b[39m=\u001b[39m mean(scores)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    564\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    565\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    566\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    567\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    568\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    569\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    570\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    571\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    572\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    574\u001b[0m )\n\u001b[0;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m         clone(estimator),\n\u001b[0;32m    312\u001b[0m         X,\n\u001b[0;32m    313\u001b[0m         y,\n\u001b[0;32m    314\u001b[0m         scorers,\n\u001b[0;32m    315\u001b[0m         train,\n\u001b[0;32m    316\u001b[0m         test,\n\u001b[0;32m    317\u001b[0m         verbose,\n\u001b[0;32m    318\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    319\u001b[0m         fit_params,\n\u001b[0;32m    320\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    321\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    322\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    323\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    324\u001b[0m     )\n\u001b[0;32m    325\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m indices\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1698\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1699\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   1700\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, space_eval\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import load_wine\n",
    "from lxml import etree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cupy as cp\n",
    "from statistics import mean\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# For visual \n",
    "BOLD = \"\\033[1m\"\n",
    "END = \"\\033[0m\"\n",
    "GREEN = \"\\033[32m\"\n",
    "RED = \"\\033[31m\"\n",
    "ORANGE = \"\\033[38;5;208m\" \n",
    "BLUE = \"\\033[34m\"\n",
    "\n",
    "def load_csv_data(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_xml_label(xml_path):\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    name = root.find('.//name').text\n",
    "    return name\n",
    "\n",
    "def load_data_from_directory(data_directory):\n",
    "    X = []  # Features\n",
    "    y = []  # Labels\n",
    "\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = data_directory + '/' + filename\n",
    "            xml_path = csv_path.replace('csv', 'xml')\n",
    "            if os.path.exists(xml_path):\n",
    "                X.append(load_csv_data(csv_path))\n",
    "                y.append(load_xml_label(xml_path))\n",
    "            else:\n",
    "                print(\"File not found!\")\n",
    "    print(\"\\nLoading complete!\")\n",
    "    return X, y\n",
    "\n",
    "# Carica i dati dal tuo percorso di directory contenente sia i file .csv che i file .xml\n",
    "data_directory = \"/Users/Riccardo/Desktop/Progetto Scalogrammi/Scalogrammi\"\n",
    "print(\"Loading datas from: \" + data_directory)\n",
    "X, y = load_data_from_directory(data_directory)\n",
    "print(\"Datasets dimension:\")\n",
    "print(f\"X shape: {len(X)}\")\n",
    "print(f\"y shape: {len(y)}\")\n",
    "\n",
    "\n",
    "# I miei data al momento si trovano all'interno di una lista chiamata X contenente i dataFrame creati da ps\n",
    "# per normalizzarli devo scorrere la lista e normalizzarne uno ad uno\n",
    "\n",
    "flat_X = [df.values.flatten() for df in X]\n",
    "X = np.vstack(flat_X)\n",
    "\n",
    "n = 0\n",
    "generations = 100\n",
    "best_accuracy = 0\n",
    "accuracy_test_best = 0\n",
    "random_saver = 0\n",
    "\n",
    "while n < generations:\n",
    "\n",
    "    random_seed = random.randint(1, 10000)\n",
    "\n",
    "    # Split dei dati in training set e test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_seed)\n",
    "    print(\"\\nData splitted into training, validation and test sets\")\n",
    "    # Creazione del validation set\n",
    "    X_train_2, X_val_temp, y_train_temp, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=random_seed)\n",
    "\n",
    "    # Normalizzazione dei dati di training e validation\n",
    "    scaler = StandardScaler()\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    X_test_norm = scaler.fit_transform(X_test)\n",
    "    X_train_temp = scaler.fit_transform(X_train_2)\n",
    "    X_val = scaler.fit_transform(X_val_temp)\n",
    "\n",
    "    np.savetxt('X_train_norm.csv', X_train_norm, delimiter=',') # <---| Salvo i dati per effettuare un controllo su di essi e che non vi siano irregolarità |\n",
    "\n",
    "    # Creiamo il classificatore SVM\n",
    "    svm_classifier = SVC(random_state=random_seed) #Classifier\n",
    "\n",
    "    # param_grid for bayesian optimization\n",
    "    param_grid_bs = {\n",
    "\n",
    "        'C': hp.loguniform('C', -3, 3),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']), # Tipo di kernel utilizzato dalla SVM [linear, poly, rbf, sigmoid, precomputed]\n",
    "        'gamma': hp.loguniform('gamma', -3, 3), # Aggiungi i valori per l'iperparametro \"gamma\"\n",
    "        # 'C': hp.uniform('C', 1.5, 3), altri parametri\n",
    "        # 'gamma': hp.uniform('gamma', 9, 11), altri parametri\n",
    "    }\n",
    "\n",
    "    scoring = ['accuracy']\n",
    "\n",
    "    # Bayesian optimization for Hyper-parameters tuning\n",
    "\n",
    "    # Funzione obiettivo\n",
    "    def objective(param, X, y):\n",
    "\n",
    "        svc = SVC(**param)\n",
    "        scores = cross_val_score(svc, X, y, cv=5, scoring='accuracy', n_jobs=-1, error_score='raise') # Cross-validation = 5, n_jobs = -1 means use all cores avaiables\n",
    "\n",
    "        # best\n",
    "        best_score = mean(scores)\n",
    "\n",
    "        # min loss\n",
    "        loss = -best_score\n",
    "\n",
    "        return {'loss':loss, 'params':param, 'status':STATUS_OK}\n",
    "\n",
    "\n",
    "    # Trials tracking process\n",
    "    bayes_trials = Trials() \n",
    "    \n",
    "    '''The model-based optimization algorithms used by hyperopt's fmin function work by analyzing samples of a response surface--a\n",
    "      history of what points in the search space were tested, and what was discovered by those tests. \n",
    "      A Trials instance stores that history and makes it available to fmin and to the various optimization algorithms.'''\n",
    "\n",
    "    print(f'\\n\\nIteration {n + 1}: - Random Seed: {random_seed}')\n",
    "\n",
    "    # Optimization\n",
    "    best = fmin(fn = lambda param: objective(param, X_train_temp, y_train_temp), #Pass X_train, y_train as parameters to objective\n",
    "                    space = param_grid_bs, algo = tpe.suggest, max_evals = 220, trials = bayes_trials)\n",
    "\n",
    "    # index best parameters\n",
    "    print(best)\n",
    "\n",
    "    # value best parameters\n",
    "    print(space_eval(param_grid_bs, best))\n",
    "\n",
    "    # Training using the best parameters found by the bayesian optimization\n",
    "    svc_bs = SVC(C=space_eval(param_grid_bs, best)['C'], gamma=space_eval(param_grid_bs, best)['gamma'], kernel=space_eval(param_grid_bs, best)['kernel']).fit(X_train_temp, y_train_temp)\n",
    "\n",
    "    # print best accuracy for testing\n",
    "    accuracy_val = svc_bs.score(X_val, y_val)\n",
    "    print(f'The accuracy score for the validation dataset is {accuracy_val:.4f}') \n",
    "\n",
    "    # All-time best validation set score\n",
    "    if accuracy_val > best_accuracy:\n",
    "        best_accuracy = accuracy_val\n",
    "        best_hyperparameters = space_eval(param_grid_bs, best)\n",
    "        accuracy_part = f'{BOLD}{GREEN}The best accuracy on the validation set is {END}'\n",
    "        accuracy_value = f'{BOLD}{RED}{best_accuracy:.4f} {END}'\n",
    "        hyperparameters_part = f'{BOLD}{GREEN}with hyperparameters: {END}'\n",
    "        hyperparameters_value = f'{BOLD}{RED}{best_hyperparameters}{END}'\n",
    "        random_seed_part = f'{BOLD}{GREEN}with random seed: {END}'\n",
    "        random_seed_value = f'{BOLD}{GREEN}{random_seed}{END}'\n",
    "        # Found a new best: best_accuracy = 0.9247, with hyperparameters: {'C': 1.661179450996574, 'gamma': 10.643118612567408, 'kernel': 'poly'}\n",
    "        # just a comment with a good accuracy\n",
    "\n",
    "        formatted_sentence = accuracy_part + accuracy_value + hyperparameters_part + hyperparameters_value + random_seed_part + random_seed_value\n",
    "\n",
    "        print(formatted_sentence)\n",
    "\n",
    "    svc_test = SVC(C = best_hyperparameters['C'], gamma = best_hyperparameters['gamma'], kernel = best_hyperparameters['kernel'])\n",
    "    svc_test.fit(X_train_norm, y_train)\n",
    "    y_pred = svc_test.predict(X_test_norm)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred)\n",
    "    print(f'\\nThe accuracy score on the test dataset, with this hyper-parameters is {BOLD}{BLUE}{accuracy_test:.4f}{END}')\n",
    "\n",
    "    \n",
    "    # All-time best test set score\n",
    "    if accuracy_test > accuracy_test_best:\n",
    "        accuracy_test_best = accuracy_test\n",
    "        best_hyperparameters_test = best_hyperparameters\n",
    "        print(f'\\n{BOLD}{ORANGE}THAT\\'S A NEW BEST!!{END}')\n",
    "        random_saver = random_seed\n",
    "\n",
    "\n",
    "    n = n + 1\n",
    "    ###############################################################################################\n",
    "\n",
    "    # 300:'C': 1.6530246210180544, 'gamma': 9.301052005520031, 'kernel': 'poly', accuracy 0.9022\n",
    "    # Evaluate the best hyperparam on the test data set\n",
    "\n",
    "\n",
    "#Salvataggio degli iperprametri con la maggior accuratezza trovata dal bayesian\n",
    "param_grid_bs_str = json.dumps(best_hyperparameters_test)\n",
    "file_name = \"best_result.txt\"\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(\"\\n\" + str(len(X)) + \": \" + param_grid_bs_str + \", accuracy [\" + str(accuracy_test_best) + \"], random seed [\" + str(random_saver) + ']')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SOLO FIT CON VISUALIZZAZIONE GRAFICA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datas from: /Users/Riccardo/Desktop/Progetto Scalogrammi/Scalogrammi\n",
      "\n",
      "Loading complete!\n",
      "Datasets dimension:\n",
      "X shape: 618\n",
      "y shape: 618\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "The accuracy score on the test dataset is \u001b[1m\u001b[34m0.9097\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, space_eval\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import load_wine\n",
    "from lxml import etree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cupy as cp\n",
    "from statistics import mean\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# For visual \n",
    "BOLD = \"\\033[1m\"\n",
    "END = \"\\033[0m\"\n",
    "GREEN = \"\\033[32m\"\n",
    "RED = \"\\033[31m\"\n",
    "ORANGE = \"\\033[38;5;208m\" \n",
    "BLUE = \"\\033[34m\"\n",
    "\n",
    "def load_csv_data(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_xml_label(xml_path):\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    name = root.find('.//name').text\n",
    "    return name\n",
    "\n",
    "def load_data_from_directory(data_directory):\n",
    "    X = []  # Features\n",
    "    y = []  # Labels\n",
    "\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = data_directory + '/' + filename\n",
    "            xml_path = csv_path.replace('csv', 'xml')\n",
    "            if os.path.exists(xml_path):\n",
    "                X.append(load_csv_data(csv_path))\n",
    "                y.append(load_xml_label(xml_path))\n",
    "            else:\n",
    "                print(\"File not found!\")\n",
    "    print(\"\\nLoading complete!\")\n",
    "    return X, y\n",
    "\n",
    "# Carica i dati dal tuo percorso di directory contenente sia i file .csv che i file .xml\n",
    "data_directory = \"/Users/Riccardo/Desktop/Progetto Scalogrammi/Scalogrammi\"\n",
    "print(\"Loading datas from: \" + data_directory)\n",
    "X, y = load_data_from_directory(data_directory)\n",
    "print(\"Datasets dimension:\")\n",
    "print(f\"X shape: {len(X)}\")\n",
    "print(f\"y shape: {len(y)}\")\n",
    "\n",
    "\n",
    "# I miei data al momento si trovano all'interno di una lista chiamata X contenente i dataFrame creati da ps\n",
    "# per normalizzarli devo scorrere la lista e normalizzarne uno ad uno\n",
    "\n",
    "\n",
    "flat_X = [df.values.flatten() for df in X]\n",
    "X = np.vstack(flat_X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7930)\n",
    "print(\"\\nData splitted into training, validation and test sets\")\n",
    "\n",
    "# Normalizing\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.fit_transform(X_test)\n",
    "\n",
    "# Fitting with best hyperparameters found in previous code \n",
    "'''618:{'C': 3.764180544055172, 'gamma': 0.09993190961266894, 'kernel': 'poly'}, random_state = 7930''' \n",
    "# Creating classifier\n",
    "svc_test = SVC(C = 3.764180544055172, gamma = 0.09993190961266894, kernel = \"poly\", random_state=7930)\n",
    "svc_test.fit(X_train_norm, y_train)\n",
    "\n",
    "# Predicition\n",
    "y_pred = svc_test.predict(X_test_norm)\n",
    "\n",
    "# Creating Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nThe accuracy score on the test dataset is {BOLD}{BLUE}{accuracy_test:.4f}{END}')\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=svc_test.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****SENZA LOOP****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datas from: /Users/Riccardo/Desktop/Progetto Scalogrammi/Scalogrammi\n",
      "\n",
      "Loading complete!\n",
      "Datasets dimension:\n",
      "X shape: 618\n",
      "y shape: 618\n",
      "\n",
      "Data splitted into training, validation and test sets\n",
      "\n",
      "Starting normalization of each dataFrame (train, test)\n",
      "\n",
      "Normalization completed!\n",
      "100%|██████████| 500/500 [20:48<00:00,  2.50s/trial, best loss: -0.8405063291139241]\n",
      "{'C': 2.4877215934325068, 'gamma': 5.234752627077685, 'kernel': 1}\n",
      "{'C': 2.4877215934325068, 'gamma': 5.234752627077685, 'kernel': 'poly'}\n",
      "The accuracy score for the validation dataset is 0.7677\n",
      "\u001b[1m\u001b[32mThe best accuracy on the validation set is \u001b[0m\u001b[1m\u001b[31m0.7677 \u001b[0m\u001b[1m\u001b[32mwith hyperparameters: \u001b[0m\u001b[1m\u001b[31m{'C': 2.4877215934325068, 'gamma': 5.234752627077685, 'kernel': 'poly'}\u001b[0m\n",
      "\n",
      "The accuracy score on the test dataset, with this hyper-parameters is \u001b[1m\u001b[34m0.7984\u001b[0m\n",
      "\n",
      " \u001b[1m\u001b[38;5;208mTHAT'S A NEW BEST!!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, space_eval\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from lxml import etree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cupy as cp\n",
    "from statistics import mean\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "# For visual \n",
    "BOLD = \"\\033[1m\"\n",
    "END = \"\\033[0m\"\n",
    "GREEN = \"\\033[32m\"\n",
    "RED = \"\\033[31m\"\n",
    "ORANGE = \"\\033[38;5;208m\" \n",
    "BLUE = \"\\033[34m\"\n",
    "\n",
    "def load_csv_data(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_xml_label(xml_path):\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    name = root.find('.//name').text\n",
    "    return name\n",
    "\n",
    "def load_data_from_directory(data_directory):\n",
    "    X = []  # Features\n",
    "    y = []  # Labels\n",
    "\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = data_directory + '/' + filename\n",
    "            xml_path = csv_path.replace('csv', 'xml')\n",
    "            if os.path.exists(xml_path):\n",
    "                X.append(load_csv_data(csv_path))\n",
    "                y.append(load_xml_label(xml_path))\n",
    "            else:\n",
    "                print(\"File not found!\")\n",
    "    print(\"\\nLoading complete!\")\n",
    "    return X, y\n",
    "\n",
    "# Carica i dati dal tuo percorso di directory contenente sia i file .csv che i file .xml\n",
    "data_directory = \"/Users/Riccardo/Desktop/Progetto Scalogrammi/Scalogrammi\"\n",
    "print(\"Loading datas from: \" + data_directory)\n",
    "X, y = load_data_from_directory(data_directory)\n",
    "print(\"Datasets dimension:\")\n",
    "print(f\"X shape: {len(X)}\")\n",
    "print(f\"y shape: {len(y)}\")\n",
    "\n",
    "\n",
    "# I miei data al momento si trovano all'interno di una lista chiamata X contenente i dataFrame creati da ps\n",
    "# per normalizzarli devo scorrere la lista e normalizzarne uno ad uno\n",
    "\n",
    "flat_X = [df.values.flatten() for df in X]\n",
    "X = np.vstack(flat_X)\n",
    "\n",
    "random_seed = random.randint(1, 10000)\n",
    "\n",
    "# Split dei dati in training set e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_seed)\n",
    "X_train_2, X_val_temp, y_train_temp, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=random_seed)\n",
    "print(\"\\nData splitted into training, validation and test sets\")\n",
    "\n",
    "# Utilizziamo la TF-IDF per vettorizzare i dati CSV\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "# X_val_vectorized = vectorizer.transform(X_val)\n",
    "# X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "#Normalizzo i parametri con vettorizzazione\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train_vectorized.toarray())  # Converto da matrice ad array\n",
    "# X_val_scaled = scaler.transform(X_val_vectorized.toarray()) # Converto da matrice ad array\n",
    "# X_test_scaled = scaler.transform(X_test_vectorized.toarray())\n",
    "\n",
    "\n",
    "# Normalizzazione senza vettorizzazione\n",
    "print(\"\\nStarting normalization of each dataFrame (train, test)\")\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.fit_transform(X_test)\n",
    "X_train_temp = scaler.fit_transform(X_train_2)\n",
    "X_val = scaler.fit_transform(X_val_temp)\n",
    "\n",
    "print(\"\\nNormalization completed!\")\n",
    "\n",
    "np.savetxt('X_train_norm.csv', X_train_norm, delimiter=',')\n",
    "\n",
    "# print(\"Normalization completed successfully!\")\n",
    "# Creiamo il classificatore SVM\n",
    "\n",
    "svm_classifier = SVC(random_state=random_seed) #Classifier\n",
    "\n",
    "# Definisco gli iperparametri \n",
    "gamma_range = np.logspace(-10, 10, 21)\n",
    "C_range = np.logspace(-10, 10, 21)\n",
    "\n",
    "# param_grid for bayesian optimization\n",
    "param_grid_bs = {\n",
    "\n",
    "    'C': hp.loguniform('C', -3, 3),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']), # Tipo di kernel utilizzato dalla SVM\n",
    "    # linear, poly, rbf, sigmoid, precomputed \n",
    "    'gamma': hp.loguniform('gamma', -3, 3), # Aggiungi i valori per l'iperparametro \"gamma\"\n",
    "\n",
    "}\n",
    "\n",
    "scoring = ['accuracy']\n",
    "\n",
    "# Cross validation\n",
    "################################################################################################\n",
    "# Questo frammento di codice è utilizzato per una K-Fold cross-validation\n",
    "# k_fold = sk.model_selection.KFold(n_splits=3, shuffle=True)\n",
    "# grid_search = GridSearchCV(svm_classifier, param_grid, cv=k_fold)\n",
    "\n",
    "\n",
    "# Questo frammento di codice è utilizzato per la cross-validation classica \n",
    "# Cerco di trovare i migliori iperparametri attraverso una cross-validation sul training set  \n",
    "#   \n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=32)  \n",
    "\n",
    "# grid_search = GridSearchCV(svm_classifier, param_grid, cv=kfold, refit='accuracy', n_jobs=-1, scoring=scoring, verbose=3) # Verbose stampa il tempo di computazione per ogni fol\n",
    "\n",
    "\n",
    "# grid_result = grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# Bayesian optimization for Hyper-parameters tuning\n",
    "\n",
    "# Funzione obiettivo\n",
    "def objective(param, X, y):\n",
    "\n",
    "    svc = SVC(**param)\n",
    "    scores = cross_val_score(svc, X, y, cv=5, scoring='accuracy', n_jobs=-1, error_score='raise')\n",
    "\n",
    "    # best\n",
    "    best_score = mean(scores)\n",
    "\n",
    "    # min loss\n",
    "    loss = -best_score\n",
    "\n",
    "    return {'loss':loss, 'params':param, 'status':STATUS_OK}\n",
    "\n",
    "\n",
    "# Trials tracking process\n",
    "bayes_trials = Trials()\n",
    "\n",
    "\n",
    "# Optimization\n",
    "best = fmin(fn = lambda param: objective(param, X_train_temp, y_train_temp), #Pass X_train, y_train as parameters to objective\n",
    "                 space = param_grid_bs, algo = tpe.suggest, max_evals = 500, trials = bayes_trials)\n",
    "\n",
    "# index best parameters\n",
    "print(best)\n",
    "\n",
    "# value best parameters\n",
    "print(space_eval(param_grid_bs, best))\n",
    "\n",
    "# Training using the best parameters found by the bayesian optimization\n",
    "svc_bs = SVC(C=space_eval(param_grid_bs, best)['C'], gamma=space_eval(param_grid_bs, best)['gamma'], kernel=space_eval(param_grid_bs, best)['kernel']).fit(X_train_temp, y_train_temp)\n",
    "\n",
    "# print best accuracy for testing\n",
    "accuracy_val = svc_bs.score(X_val, y_val)\n",
    "print(f'The accuracy score for the validation dataset is {accuracy_val:.4f}') \n",
    "\n",
    "\n",
    "best_accuracy = accuracy_val\n",
    "best_hyperparameters = space_eval(param_grid_bs, best)\n",
    "accuracy_part = f'{BOLD}{GREEN}The best accuracy on the validation set is {END}'\n",
    "accuracy_value = f'{BOLD}{RED}{best_accuracy:.4f} {END}'\n",
    "hyperparameters_part = f'{BOLD}{GREEN}with hyperparameters: {END}'\n",
    "hyperparameters_value = f'{BOLD}{RED}{best_hyperparameters}{END}'\n",
    "# Found a new best: best_accuracy = 0.9247, with hyperparameters: {'C': 1.661179450996574, 'gamma': 10.643118612567408, 'kernel': 'poly'}\n",
    "# just a comment with a good accuracy\n",
    "\n",
    "formatted_sentence = accuracy_part + accuracy_value + hyperparameters_part + hyperparameters_value\n",
    "\n",
    "print(formatted_sentence)\n",
    "\n",
    "svc_test = SVC(C = best_hyperparameters['C'], gamma = best_hyperparameters['gamma'], kernel = best_hyperparameters['kernel'])\n",
    "svc_test.fit(X_train_norm, y_train)\n",
    "y_pred = svc_test.predict(X_test_norm)\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nThe accuracy score on the test dataset, with this hyper-parameters is {BOLD}{BLUE}{accuracy_test:.4f}{END}')\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# 300:'C': 1.6530246210180544, 'gamma': 9.301052005520031, 'kernel': 'poly', accuracy 0.9022\n",
    "# Evaluate the best hyperparam on the test data set\n",
    "\n",
    "best_accuracy_test = 0\n",
    "#Salvataggio degli iperprametri con la maggior accuratezza trovata dal bayesian\n",
    "param_grid_bs_str = json.dumps(best_hyperparameters)\n",
    "file_name = \"best_result.txt\"\n",
    "with open(file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        accuracy_match = re.search(r'accuracy ([\\d.]+)', line)\n",
    "        if accuracy_match:\n",
    "            accuracy_in_file = float(accuracy_match.group(1))\n",
    "            if accuracy_test > accuracy_in_file:\n",
    "                best_accuracy_test = accuracy_test\n",
    "                print(f'\\n {BOLD}{ORANGE}THAT\\'S A NEW BEST!!{END}')\n",
    "                break\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(\"\\n\" + str(len(X)) + \": \" + param_grid_bs_str + \", accuracy \" + str(accuracy_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datas from: /Users/Riccardo/Desktop/Progetto Scalogrammi/Scalogrammi\n",
      "\n",
      "Loading complete!\n",
      "Datasets dimension:\n",
      "X shape: 618\n",
      "y shape: 618\n",
      "\n",
      "Data splitted into training and test sets\n",
      "\n",
      "Starting normalization of each dataFrame (train, test)\n",
      "\n",
      "...\n",
      "\n",
      "Normalization completed!\n",
      "100%|██████████| 600/600 [19:24<00:00,  1.94s/trial, best loss: -0.7991481636642928]\n",
      "{'C': 1.9418042739186285, 'gamma': 10.407294205118141, 'kernel': 1}\n",
      "{'C': 1.9418042739186285, 'gamma': 10.407294205118141, 'kernel': 'poly'}\n",
      "The accuracy score for the testing dataset is 0.8968\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, space_eval\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from lxml import etree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cupy as cp\n",
    "from statistics import mean\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "\n",
    "def load_csv_data(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_xml_label(xml_path):\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    name = root.find('.//name').text\n",
    "    return name\n",
    "\n",
    "def load_data_from_directory(data_directory):\n",
    "    X = []  # Features\n",
    "    y = []  # Labels\n",
    "\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = data_directory + '/' + filename\n",
    "            xml_path = csv_path.replace('csv', 'xml')\n",
    "            if os.path.exists(xml_path):\n",
    "                X.append(load_csv_data(csv_path))\n",
    "                y.append(load_xml_label(xml_path))\n",
    "            else:\n",
    "                print(\"File not found!\")\n",
    "    print(\"\\nLoading complete!\")\n",
    "    return X, y\n",
    "\n",
    "# Carica i dati dal tuo percorso di directory contenente sia i file .csv che i file .xml\n",
    "data_directory = \"/Users/Riccardo/Desktop/Progetto Scalogrammi/Scalogrammi\"\n",
    "print(\"Loading datas from: \" + data_directory)\n",
    "X, y = load_data_from_directory(data_directory)\n",
    "print(\"Datasets dimension:\")\n",
    "print(f\"X shape: {len(X)}\")\n",
    "print(f\"y shape: {len(y)}\")\n",
    "\n",
    "\n",
    "# I miei data al momento si trovano all'interno di una lista chiamata X contenente i dataFrame creati da ps\n",
    "# per normalizzarli devo scorrere la lista e normalizzarne uno ad uno\n",
    "\n",
    "flat_X = [df.values.flatten() for df in X]\n",
    "X = np.vstack(flat_X)\n",
    "\n",
    "# Split dei dati in training set e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "print(\"\\nData splitted into training and test sets\")\n",
    "\n",
    "# Split del training set in training set e validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "# Utilizziamo la TF-IDF per vettorizzare i dati CSV\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "# X_val_vectorized = vectorizer.transform(X_val)\n",
    "# X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "#Normalizzo i parametri con vettorizzazione\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train_vectorized.toarray())  # Converto da matrice ad array\n",
    "# X_val_scaled = scaler.transform(X_val_vectorized.toarray()) # Converto da matrice ad array\n",
    "# X_test_scaled = scaler.transform(X_test_vectorized.toarray())\n",
    "\n",
    "\n",
    "# Normalizzazione senza vettorizzazione\n",
    "print(\"\\nStarting normalization of each dataFrame (train, test)\")\n",
    "scaler = StandardScaler()\n",
    "# X_val_scaled = scaler.transform(X_val) # Converto da matrice ad array\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "print(\"\\n...\")\n",
    "X_test_norm = scaler.fit_transform(X_test)\n",
    "\n",
    "print(\"\\nNormalization completed!\")\n",
    "\n",
    "np.savetxt('X_train_norm.csv', X_train_norm, delimiter=',')\n",
    "\n",
    "# print(\"Normalization completed successfully!\")\n",
    "# Creiamo il classificatore SVM\n",
    "\n",
    "svm_classifier = SVC(random_state=1) #Classifier\n",
    "\n",
    "# Definisco gli iperparametri \n",
    "gamma_range = np.logspace(-10, 10, 21)\n",
    "C_range = np.logspace(-10, 10, 21)\n",
    "\n",
    "'''\n",
    "param_grid = {\n",
    "\n",
    "    'C': np.logspace(-3, 2, 6),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], # Tipo di kernel utilizzato dalla SVM\n",
    "    # linear, poly, rbf, sigmoid, precomputed \n",
    "    'gamma': gamma_range.tolist() +  ['auto', 'scale'] # Aggiungi i valori per l'iperparametro \"gamma\"\n",
    "    \n",
    "}'''\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    'C':np.logspace(-3, 2, 6),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], # Tipo di kernel utilizzato dalla SVM\n",
    "    # linear, poly, rbf, sigmoid, precomputed \n",
    "    'gamma':  gamma_range.tolist() + ['scale', 'auto'] # Aggiungi i valori per l'iperparametro \"gamma\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# param_grid for bayesian optimization\n",
    "param_grid_bs = {\n",
    "\n",
    "    # 'C': hp.loguniform('C', -3, 3),\n",
    "    'C': hp.uniform('C', 1.5, 3),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']), # Tipo di kernel utilizzato dalla SVM\n",
    "    'gamma': hp.uniform('gamma', 9, 11),\n",
    "    # linear, poly, rbf, sigmoid, precomputed \n",
    "    # 'gamma': hp.loguniform('gamma', -3, 3), # Aggiungi i valori per l'iperparametro \"gamma\"\n",
    "\n",
    "}\n",
    "\n",
    "scoring = ['accuracy']\n",
    "\n",
    "# Cross validation\n",
    "################################################################################################\n",
    "# Questo frammento di codice è utilizzato per una K-Fold cross-validation\n",
    "# k_fold = sk.model_selection.KFold(n_splits=3, shuffle=True)\n",
    "# grid_search = GridSearchCV(svm_classifier, param_grid, cv=k_fold)\n",
    "\n",
    "\n",
    "# Questo frammento di codice è utilizzato per la cross-validation classica \n",
    "# Cerco di trovare i migliori iperparametri attraverso una cross-validation sul training set  \n",
    "#   \n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=32)  \n",
    "\n",
    "# grid_search = GridSearchCV(svm_classifier, param_grid, cv=kfold, refit='accuracy', n_jobs=-1, scoring=scoring, verbose=3) # Verbose stampa il tempo di computazione per ogni fol\n",
    "\n",
    "\n",
    "# grid_result = grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# Bayesian optimization for Hyper-parameters tuning\n",
    "\n",
    "# Funzione obiettivo\n",
    "def objective(param):\n",
    "\n",
    "    svc = SVC(**param)\n",
    "    scores = cross_val_score(svc, X_train_norm, y_train, cv=kfold, scoring='accuracy', n_jobs=-1, error_score='raise')\n",
    "\n",
    "    # best\n",
    "    best_score = mean(scores)\n",
    "\n",
    "    # min loss\n",
    "    loss = -best_score\n",
    "\n",
    "    return {'loss':loss, 'params':param, 'status':STATUS_OK}\n",
    "\n",
    "# Trials tracking process\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# Optimization\n",
    "best = fmin(fn = objective, space = param_grid_bs, algo = tpe.suggest, max_evals = 600, trials = bayes_trials)\n",
    "\n",
    "# index best parameters\n",
    "print(best)\n",
    "\n",
    "# value best parameters\n",
    "print(space_eval(param_grid_bs, best))\n",
    "\n",
    "# Training using the best parameters found by the bayesian optimization\n",
    "svc_bs = SVC(C=space_eval(param_grid_bs, best)['C'], gamma=space_eval(param_grid_bs, best)['gamma'], kernel=space_eval(param_grid_bs, best)['kernel']).fit(X_train_norm, y_train)\n",
    "\n",
    "# print best accuracy for testing\n",
    "accuracy_test = svc_bs.score(X_test_norm, y_test)\n",
    "print(f'The accuracy score for the testing dataset is {accuracy_test:.4f}') \n",
    "###############################################################################################\n",
    "\n",
    "# 300:'C': 1.6530246210180544, 'gamma': 9.301052005520031, 'kernel': 'poly', accuracy 0.9022\n",
    "\n",
    "#Salvataggio degli iperprametri con la maggior accuratezza trovata dal bayesian\n",
    "param_grid_bs_str = json.dumps(space_eval(param_grid_bs, best))\n",
    "file_name = \"best_result.txt\"\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(\"\\n\" + str(len(X)) + \": \" + param_grid_bs_str + \", accuracy \" + str(accuracy_test))\n",
    "\n",
    "# Print the best accuracy score for the training dataset\n",
    "## print(f'The best accuracy score for the training dataset is {grid_result.best_score_:.4f}')\n",
    "# Print the hyperparameters for the best score\n",
    "## print(f'The best hyperparameters are {grid_result.best_params_}')\n",
    "# Print the best accuracy score for the testing dataset\n",
    "## print(f'The accuracy score for the testing dataset is {grid_search.score(X_test_norm, y_test):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
